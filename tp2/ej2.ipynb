{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be78b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de NaNs en titleSentiment 26\n",
      "Cantidad de registros 257\n",
      "Cantidad de registros despues de sacar NaNs: 231.\n",
      "     starRating  wordcount  titleSentiment  sentimentValue\n",
      "0             1         20             0.0       -0.486389\n",
      "1             1          6             0.0       -0.586187\n",
      "4             1          6             0.0       -0.651784\n",
      "5             1          8             1.0       -0.720443\n",
      "6             1         11             1.0       -0.726825\n",
      "7             1         16             1.0       -0.736769\n",
      "8             1          3             1.0       -0.765284\n",
      "9             1         13             0.0       -0.797961\n",
      "10            1          4             1.0       -0.833488\n",
      "11            1          9             0.0       -0.838467\n",
      "12            1          2             1.0       -0.888559\n",
      "13            1         23             0.0       -1.002696\n",
      "14            1          7             0.0       -1.083269\n",
      "15            1         10             1.0       -1.098110\n",
      "16            1          1             0.0       -1.104469\n",
      "17            1          7             1.0       -1.112223\n",
      "18            1          5             1.0       -1.275411\n",
      "19            1          5             1.0       -1.286009\n",
      "20            1          3             1.0       -1.455640\n",
      "21            1          7             0.0       -1.479433\n",
      "22            1         10             0.0       -1.498306\n",
      "23            1          3             1.0       -1.780889\n",
      "24            1          2             1.0       -2.276469\n",
      "25            1         29             1.0        0.107671\n",
      "26            2         15             0.0       -0.062616\n",
      "27            2         11             1.0       -0.063282\n",
      "28            2         28             0.0       -0.130138\n",
      "29            2          8             1.0       -0.201710\n",
      "31            2          3             0.0       -0.552986\n",
      "32            2         15             0.0       -0.615022\n",
      "33            2          6             0.0       -0.642228\n",
      "34            2          7             1.0       -0.747805\n",
      "35            2         27             1.0       -0.769309\n",
      "36            2         17             0.0       -0.950121\n",
      "37            2          8             0.0       -1.707268\n",
      "38            2          1             0.0        0.052841\n",
      "39            2         20             1.0        0.057928\n",
      "40            2          8             1.0        0.073779\n",
      "42            3          7             1.0       -0.015019\n",
      "43            3          4             1.0       -0.022035\n",
      "44            3          2             1.0       -0.036155\n",
      "45            3          6             1.0       -0.048439\n",
      "46            3         14             1.0       -0.061755\n",
      "47            3          3             1.0       -0.065483\n",
      "48            3          6             1.0       -0.070441\n",
      "49            3          2             1.0       -0.074483\n",
      "50            3          1             1.0       -0.083467\n",
      "51            3          1             1.0       -0.083467\n",
      "52            3          2             1.0       -0.088824\n",
      "53            3          2             1.0       -0.089639\n",
      "54            3          4             1.0       -0.092401\n",
      "55            3          2             1.0       -0.096387\n",
      "56            3          9             1.0       -0.100422\n",
      "57            3          3             1.0       -0.106498\n",
      "58            3          7             1.0       -0.107456\n",
      "59            3          3             1.0       -0.107755\n",
      "60            3          5             1.0       -0.108144\n",
      "62            3          4             1.0       -0.114317\n",
      "63            3          1             1.0       -0.116132\n",
      "64            3          5             0.0       -0.137943\n",
      "65            3          1             1.0       -0.139861\n",
      "66            3          1             1.0       -0.139861\n",
      "69            3          1             1.0       -0.171701\n",
      "70            3          1             1.0       -0.213092\n",
      "71            3          7             1.0       -0.227463\n",
      "72            3          2             1.0       -0.236295\n",
      "73            3          3             1.0       -0.246730\n",
      "74            1          3             1.0       -0.256026\n",
      "75            3          4             0.0       -0.256112\n",
      "76            3          9             1.0       -0.257639\n",
      "77            3          1             0.0       -0.300491\n",
      "78            1          3             1.0       -0.311820\n",
      "79            1          1             1.0       -0.348733\n",
      "80            1         11             0.0       -0.355790\n",
      "81            1          3             1.0       -0.365337\n",
      "82            1         14             0.0       -0.376121\n",
      "84            3         28             1.0       -0.406623\n",
      "85            1          5             0.0       -0.439639\n",
      "86            3          5             1.0       -0.446730\n",
      "87            1          3             1.0       -0.465646\n",
      "88            1          4             0.0       -0.466102\n",
      "89            1         17             0.0       -0.493784\n",
      "91            3          4             1.0        0.004820\n",
      "92            3          4             1.0        0.020340\n",
      "93            3         19             1.0        0.034394\n",
      "94            3          7             1.0        0.035112\n",
      "95            3          2             1.0        0.039896\n",
      "96            3          3             1.0        0.042369\n",
      "97            3          5             1.0        0.048475\n",
      "98            3          5             1.0        0.066694\n",
      "99            3          6             1.0        0.078157\n",
      "100           3          2             1.0        0.081264\n",
      "101           3          3             1.0        0.104674\n",
      "103           3         12             1.0        0.132087\n",
      "104           3          2             1.0        0.136207\n",
      "106           3          1             1.0        0.151900\n",
      "107           3          1             1.0        0.153141\n",
      "108           3          3             1.0        0.157436\n",
      "109           3          6             1.0        0.160616\n",
      "110           3          4             1.0        0.170651\n",
      "111           3          2             1.0        0.190015\n",
      "112           3          3             1.0        0.201199\n",
      "113           3          5             1.0        0.212674\n",
      "114           3          3             1.0        0.218230\n",
      "115           3          4             1.0        0.237717\n",
      "116           3          1             1.0        0.238396\n",
      "117           3         11             1.0        0.244713\n",
      "118           3          6             1.0        0.246869\n",
      "119           3          3             1.0        0.281624\n",
      "120           3          3             1.0        0.283673\n",
      "121           3          1             1.0        0.288224\n",
      "122           3          5             1.0        0.297272\n",
      "125           3          5             0.0        0.301521\n",
      "126           3          4             1.0        0.308424\n",
      "127           3          2             1.0        0.310468\n",
      "128           3          2             1.0        2.066294\n",
      "129           3          1             1.0        2.494902\n",
      "130           4          7             1.0        0.107111\n",
      "131           2         17             1.0        0.136766\n",
      "132           2          5             1.0        0.145602\n",
      "133           2         26             1.0        0.157160\n",
      "134           2          5             0.0        0.193291\n",
      "136           2          6             1.0        0.214543\n",
      "137           2         26             1.0        0.226819\n",
      "138           4          7             0.0        0.236713\n",
      "139           2         17             1.0        0.264091\n",
      "141           4          4             1.0        0.315914\n",
      "142           2          9             0.0        0.319576\n",
      "143           2         12             1.0        0.358095\n",
      "144           4          6             1.0        0.397294\n",
      "145           4          6             1.0        0.400986\n",
      "146           4          6             1.0        0.472362\n",
      "147           4         10             1.0        0.511459\n",
      "148           4          3             1.0        0.513035\n",
      "149           4          5             1.0        0.528874\n",
      "150           4          5             1.0        0.581934\n",
      "151           4         12             0.0        0.743365\n",
      "152           4         13             1.0        0.808384\n",
      "153           4          5             1.0        0.813521\n",
      "154           4          9             1.0        0.870911\n",
      "155           4         14             1.0        0.911598\n",
      "156           4         10             1.0        2.078533\n",
      "157           5          3             1.0        0.319412\n",
      "158           5          1             1.0        0.335548\n",
      "159           5          6             1.0        0.340636\n",
      "160           5          1             1.0        0.340838\n",
      "161           5          7             1.0        0.344929\n",
      "162           5          1             1.0        0.373013\n",
      "163           5          4             1.0        0.377286\n",
      "164           5          1             1.0        0.410584\n",
      "165           4          7             1.0        0.416996\n",
      "166           5          8             1.0        0.436012\n",
      "167           4          6             1.0        0.458070\n",
      "168           5          6             1.0        0.462592\n",
      "169           5          7             0.0        0.475378\n",
      "170           4         12             1.0        0.502011\n",
      "171           5          2             1.0        0.531343\n",
      "172           5          2             1.0        0.556533\n",
      "173           4          2             1.0        0.590758\n",
      "174           5          5             1.0        0.598366\n",
      "175           4         12             1.0        0.626541\n",
      "177           5          6             1.0        0.644797\n",
      "178           5          3             1.0        0.647472\n",
      "179           5          2             1.0        0.650792\n",
      "180           5          5             1.0        0.651691\n",
      "181           5          9             1.0        0.660120\n",
      "182           4          2             1.0        0.664222\n",
      "183           5          5             1.0        0.666522\n",
      "184           5          3             1.0        0.670468\n",
      "185           5          3             1.0        0.680167\n",
      "186           5          6             1.0        0.681880\n",
      "187           5          3             0.0        0.681937\n",
      "188           5         10             0.0        0.693039\n",
      "189           5          4             1.0        0.694613\n",
      "190           5          3             1.0        0.703685\n",
      "192           5          2             1.0        0.729001\n",
      "193           5          2             1.0        0.736485\n",
      "194           5          6             1.0        0.777594\n",
      "195           5          2             1.0        0.789284\n",
      "196           5          2             1.0        0.789284\n",
      "197           5          9             1.0        0.794211\n",
      "198           5          9             1.0        0.796459\n",
      "199           5          6             1.0        0.825006\n",
      "200           5          5             1.0        0.848620\n",
      "202           5          3             1.0        0.950245\n",
      "204           4         11             1.0        0.970350\n",
      "207           4         10             1.0        0.987533\n",
      "208           5          4             1.0        1.015807\n",
      "209           5          8             1.0        1.034916\n",
      "210           5          6             1.0        1.080510\n",
      "211           5          5             1.0        1.112476\n",
      "212           4         11             1.0        1.117629\n",
      "213           5          4             1.0        1.132486\n",
      "214           5          1             1.0        1.165488\n",
      "215           5          1             1.0        1.165488\n",
      "216           5          7             1.0        1.173869\n",
      "217           5          3             1.0        1.175564\n",
      "218           5          2             1.0        1.204502\n",
      "219           5          5             1.0        1.213158\n",
      "220           5          1             1.0        1.213942\n",
      "221           4         14             1.0        1.252075\n",
      "222           5          2             1.0        1.271260\n",
      "223           5          3             1.0        1.294011\n",
      "224           5          4             1.0        1.362262\n",
      "225           5          2             1.0        1.379530\n",
      "226           5          2             1.0        1.379530\n",
      "228           5          3             0.0        1.405697\n",
      "229           5          9             1.0        1.413723\n",
      "230           5          2             1.0        1.422714\n",
      "231           5          7             1.0        1.425077\n",
      "232           5          5             1.0        1.496927\n",
      "234           5          2             1.0        1.593301\n",
      "236           5          3             1.0        1.630900\n",
      "237           4          5             1.0        1.769818\n",
      "238           5          1             1.0        1.837082\n",
      "239           5          5             1.0        1.881806\n",
      "240           5          3             1.0        1.943862\n",
      "241           5          3             1.0        1.943862\n",
      "242           5          2             1.0        2.052932\n",
      "243           5          7             1.0        2.055556\n",
      "244           5          2             1.0        2.066294\n",
      "245           5          2             1.0        2.066294\n",
      "247           5          5             1.0        2.098547\n",
      "248           5          1             1.0        2.220387\n",
      "249           5          2             1.0        2.295086\n",
      "251           5          1             1.0        2.494902\n",
      "252           5          3             1.0        2.814818\n",
      "253           5          1             1.0        2.924393\n",
      "254           5          1             1.0        2.924393\n",
      "255           5          1             1.0        2.924393\n",
      "256           5          3             0.0        3.264579\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.knn import KNN, WeightedKNN\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "\n",
    "attributes = ['wordcount', 'titleSentiment', 'sentimentValue']\n",
    "target = ['starRating']\n",
    "\n",
    "reviews_sentiment = pd.read_csv('datasets/reviews_sentiment.csv', delimiter=\";\")\n",
    "\n",
    "for i, row in reviews_sentiment.iterrows():\n",
    "    wc = len(row['reviewText'].strip().split(' '))\n",
    "    if wc != row['wordcount']:\n",
    "#         print(f\"El registro {i} tiene el campo wordcount incorrecto, corrigiendo...\")\n",
    "        reviews_sentiment.at[i,'wordcount'] = wc\n",
    "\n",
    "# print(sentiments)\n",
    "\n",
    "# cambiar positivo a 1, negativo a 0.\n",
    "reviews_sentiment['titleSentiment'].replace('positive', 1, inplace=True)\n",
    "reviews_sentiment['titleSentiment'].replace('negative', 0, inplace=True)\n",
    "\n",
    "# quitar los NaNs y verificar que no sea una porcion importante del conjunto de datos\n",
    "nan_title_sentiment = reviews_sentiment[reviews_sentiment['titleSentiment'].isna()]\n",
    "nan_text_sentiment = reviews_sentiment[reviews_sentiment['textSentiment'].isna()]\n",
    "print(f\"Cantidad de NaNs en titleSentiment {len(nan_title_sentiment)}\")\n",
    "print(f\"Cantidad de registros {len(reviews_sentiment)}\")\n",
    "reviews_sentiment = reviews_sentiment.dropna()\n",
    "print(f\"Cantidad de registros despues de sacar NaNs: {len(reviews_sentiment)}.\")\n",
    "\n",
    "reviews_sentiment = reviews_sentiment[target + attributes]\n",
    "\n",
    "print(reviews_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8697cbcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los comentarios valorados con 1 estrella, ¿que cantidad promedio de palabras tienen?\n",
      "El promedio es 8.06\n"
     ]
    }
   ],
   "source": [
    "# Pregunta a)\n",
    "print('Los comentarios valorados con 1 estrella, ¿que cantidad promedio de palabras tienen?')\n",
    "\n",
    "mean = reviews_sentiment[reviews_sentiment['starRating'] == 1]['wordcount'].aggregate('mean')\n",
    "print(f\"El promedio es {round(mean, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f16adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aux_functions import normalize_df\n",
    "\n",
    "classes = np.array(reviews_sentiment.starRating.unique())\n",
    "\n",
    "# print(sentiments.star_rating.unique())\n",
    "\n",
    "labels = np.array(reviews_sentiment.starRating)\n",
    "# Se normalizan los datos por cada columna\n",
    "data = np.array(normalize_df(reviews_sentiment[attributes])) # me queda un arreglo de registros con 3 atributos\n",
    "\n",
    "crossed_validation = 6\n",
    "batch_size = math.floor(len(data)/crossed_validation)\n",
    "\n",
    "# valores de precision para cada corrido de validacion cruzada\n",
    "knn_precisions = np.zeros(crossed_validation)\n",
    "weight_knn_precisions = np.zeros(crossed_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17fcbc84",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_29188/2087228526.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mweight_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mknn_confusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[0mweight_knn_confusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\MachineLearningStories\\tp2\\src\\aux_functions.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(predictions, f_X, classes)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mconf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mconf_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconf_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "from src.aux_functions import confusion_matrix\n",
    "\n",
    "for i in range(crossed_validation):\n",
    "\n",
    "    # separo por lotes al conjunto de entrenamiento/testeo para la validacion cruzada\n",
    "    test_batch = np.array(range(batch_size * i, batch_size * (i + 1), 1))\n",
    "\n",
    "    # b) Dividir el conjunto de datos en un conjunto de entrenamiento y otro de prueba.\n",
    "    X = np.delete(data, test_batch, axis = 0)\n",
    "    f_X = np.delete(labels, test_batch, axis = 0)\n",
    "    Y = data[test_batch[0]:(test_batch[-1] + 1)]\n",
    "    f_Y = labels[test_batch[0]:(test_batch[-1] + 1)]\n",
    "\n",
    "    knn = KNN(X, f_X, classes)\n",
    "    weight_knn = WeightedKNN(X, f_X, classes)\n",
    "\n",
    "    results = knn.batch_classify(Y)\n",
    "    weight_predictions = weight_knn.batch_classify(Y)\n",
    "    \n",
    "    knn_confusion = confusion_matrix(results, f_Y, classes)\n",
    "    weight_knn_confusion = confusion_matrix(weight_predictions, f_Y, classes)\n",
    "\n",
    "    knn_precisions[i] = knn_confusion.trace()/knn_confusion.sum()\n",
    "    weight_knn_precisions[i] = weight_knn_confusion.trace()/weight_knn_confusion.sum()\n",
    "    print('lalbalbl')\n",
    "    plot_heatmap(w_knn_confusion, f'w_knn_k_{crossed_validation_k}_i{i}.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
