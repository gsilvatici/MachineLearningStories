{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be78b4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7587548638132295\n",
      "Cantidad de NaNs en titleSentiment 0\n",
      "Cantidad de registros 257\n",
      "Cantidad de registros despues de sacar NaNs: 257.\n",
      "     starRating  wordcount  titleSentiment  sentimentValue\n",
      "0             1         20             0.0       -0.486389\n",
      "1             1          6             0.0       -0.586187\n",
      "2             1          4             0.5       -0.602240\n",
      "3             1         17             0.5       -0.616271\n",
      "4             1          6             0.0       -0.651784\n",
      "5             1          8             1.0       -0.720443\n",
      "6             1         11             1.0       -0.726825\n",
      "7             1         16             1.0       -0.736769\n",
      "8             1          3             1.0       -0.765284\n",
      "9             1         13             0.0       -0.797961\n",
      "10            1          4             1.0       -0.833488\n",
      "11            1          9             0.0       -0.838467\n",
      "12            1          2             1.0       -0.888559\n",
      "13            1         23             0.0       -1.002696\n",
      "14            1          7             0.0       -1.083269\n",
      "15            1         10             1.0       -1.098110\n",
      "16            1          1             0.0       -1.104469\n",
      "17            1          7             1.0       -1.112223\n",
      "18            1          5             1.0       -1.275411\n",
      "19            1          5             1.0       -1.286009\n",
      "20            1          3             1.0       -1.455640\n",
      "21            1          7             0.0       -1.479433\n",
      "22            1         10             0.0       -1.498306\n",
      "23            1          3             1.0       -1.780889\n",
      "24            1          2             1.0       -2.276469\n",
      "25            1         29             1.0        0.107671\n",
      "26            2         15             0.0       -0.062616\n",
      "27            2         11             1.0       -0.063282\n",
      "28            2         28             0.0       -0.130138\n",
      "29            2          8             1.0       -0.201710\n",
      "30            2          3             0.5       -0.295194\n",
      "31            2          3             0.0       -0.552986\n",
      "32            2         15             0.0       -0.615022\n",
      "33            2          6             0.0       -0.642228\n",
      "34            2          7             1.0       -0.747805\n",
      "35            2         27             1.0       -0.769309\n",
      "36            2         17             0.0       -0.950121\n",
      "37            2          8             0.0       -1.707268\n",
      "38            2          1             0.0        0.052841\n",
      "39            2         20             1.0        0.057928\n",
      "40            2          8             1.0        0.073779\n",
      "41            3          2             0.5       -0.012039\n",
      "42            3          7             1.0       -0.015019\n",
      "43            3          4             1.0       -0.022035\n",
      "44            3          2             1.0       -0.036155\n",
      "45            3          6             1.0       -0.048439\n",
      "46            3         14             1.0       -0.061755\n",
      "47            3          3             1.0       -0.065483\n",
      "48            3          6             1.0       -0.070441\n",
      "49            3          2             1.0       -0.074483\n",
      "50            3          1             1.0       -0.083467\n",
      "51            3          1             1.0       -0.083467\n",
      "52            3          2             1.0       -0.088824\n",
      "53            3          2             1.0       -0.089639\n",
      "54            3          4             1.0       -0.092401\n",
      "55            3          2             1.0       -0.096387\n",
      "56            3          9             1.0       -0.100422\n",
      "57            3          3             1.0       -0.106498\n",
      "58            3          7             1.0       -0.107456\n",
      "59            3          3             1.0       -0.107755\n",
      "60            3          5             1.0       -0.108144\n",
      "61            3          5             0.5       -0.110967\n",
      "62            3          4             1.0       -0.114317\n",
      "63            3          1             1.0       -0.116132\n",
      "64            3          5             0.0       -0.137943\n",
      "65            3          1             1.0       -0.139861\n",
      "66            3          1             1.0       -0.139861\n",
      "67            3          1             0.5       -0.139861\n",
      "68            3          1             0.5       -0.139861\n",
      "69            3          1             1.0       -0.171701\n",
      "70            3          1             1.0       -0.213092\n",
      "71            3          7             1.0       -0.227463\n",
      "72            3          2             1.0       -0.236295\n",
      "73            3          3             1.0       -0.246730\n",
      "74            1          3             1.0       -0.256026\n",
      "75            3          4             0.0       -0.256112\n",
      "76            3          9             1.0       -0.257639\n",
      "77            3          1             0.0       -0.300491\n",
      "78            1          3             1.0       -0.311820\n",
      "79            1          1             1.0       -0.348733\n",
      "80            1         11             0.0       -0.355790\n",
      "81            1          3             1.0       -0.365337\n",
      "82            1         14             0.0       -0.376121\n",
      "83            1          7             0.5       -0.396850\n",
      "84            3         28             1.0       -0.406623\n",
      "85            1          5             0.0       -0.439639\n",
      "86            3          5             1.0       -0.446730\n",
      "87            1          3             1.0       -0.465646\n",
      "88            1          4             0.0       -0.466102\n",
      "89            1         17             0.0       -0.493784\n",
      "90            3          8             0.5        0.001546\n",
      "91            3          4             1.0        0.004820\n",
      "92            3          4             1.0        0.020340\n",
      "93            3         19             1.0        0.034394\n",
      "94            3          7             1.0        0.035112\n",
      "95            3          2             1.0        0.039896\n",
      "96            3          3             1.0        0.042369\n",
      "97            3          5             1.0        0.048475\n",
      "98            3          5             1.0        0.066694\n",
      "99            3          6             1.0        0.078157\n",
      "100           3          2             1.0        0.081264\n",
      "101           3          3             1.0        0.104674\n",
      "102           3          2             0.5        0.125126\n",
      "103           3         12             1.0        0.132087\n",
      "104           3          2             1.0        0.136207\n",
      "105           3          4             0.5        0.136882\n",
      "106           3          1             1.0        0.151900\n",
      "107           3          1             1.0        0.153141\n",
      "108           3          3             1.0        0.157436\n",
      "109           3          6             1.0        0.160616\n",
      "110           3          4             1.0        0.170651\n",
      "111           3          2             1.0        0.190015\n",
      "112           3          3             1.0        0.201199\n",
      "113           3          5             1.0        0.212674\n",
      "114           3          3             1.0        0.218230\n",
      "115           3          4             1.0        0.237717\n",
      "116           3          1             1.0        0.238396\n",
      "117           3         11             1.0        0.244713\n",
      "118           3          6             1.0        0.246869\n",
      "119           3          3             1.0        0.281624\n",
      "120           3          3             1.0        0.283673\n",
      "121           3          1             1.0        0.288224\n",
      "122           3          5             1.0        0.297272\n",
      "123           3          1             0.5        0.297443\n",
      "124           3          3             0.5        0.297791\n",
      "125           3          5             0.0        0.301521\n",
      "126           3          4             1.0        0.308424\n",
      "127           3          2             1.0        0.310468\n",
      "128           3          2             1.0        2.066294\n",
      "129           3          1             1.0        2.494902\n",
      "130           4          7             1.0        0.107111\n",
      "131           2         17             1.0        0.136766\n",
      "132           2          5             1.0        0.145602\n",
      "133           2         26             1.0        0.157160\n",
      "134           2          5             0.0        0.193291\n",
      "135           4          5             0.5        0.199283\n",
      "136           2          6             1.0        0.214543\n",
      "137           2         26             1.0        0.226819\n",
      "138           4          7             0.0        0.236713\n",
      "139           2         17             1.0        0.264091\n",
      "140           4          3             0.5        0.272991\n",
      "141           4          4             1.0        0.315914\n",
      "142           2          9             0.0        0.319576\n",
      "143           2         12             1.0        0.358095\n",
      "144           4          6             1.0        0.397294\n",
      "145           4          6             1.0        0.400986\n",
      "146           4          6             1.0        0.472362\n",
      "147           4         10             1.0        0.511459\n",
      "148           4          3             1.0        0.513035\n",
      "149           4          5             1.0        0.528874\n",
      "150           4          5             1.0        0.581934\n",
      "151           4         12             0.0        0.743365\n",
      "152           4         13             1.0        0.808384\n",
      "153           4          5             1.0        0.813521\n",
      "154           4          9             1.0        0.870911\n",
      "155           4         14             1.0        0.911598\n",
      "156           4         10             1.0        2.078533\n",
      "157           5          3             1.0        0.319412\n",
      "158           5          1             1.0        0.335548\n",
      "159           5          6             1.0        0.340636\n",
      "160           5          1             1.0        0.340838\n",
      "161           5          7             1.0        0.344929\n",
      "162           5          1             1.0        0.373013\n",
      "163           5          4             1.0        0.377286\n",
      "164           5          1             1.0        0.410584\n",
      "165           4          7             1.0        0.416996\n",
      "166           5          8             1.0        0.436012\n",
      "167           4          6             1.0        0.458070\n",
      "168           5          6             1.0        0.462592\n",
      "169           5          7             0.0        0.475378\n",
      "170           4         12             1.0        0.502011\n",
      "171           5          2             1.0        0.531343\n",
      "172           5          2             1.0        0.556533\n",
      "173           4          2             1.0        0.590758\n",
      "174           5          5             1.0        0.598366\n",
      "175           4         12             1.0        0.626541\n",
      "176           5          4             0.5        0.634096\n",
      "177           5          6             1.0        0.644797\n",
      "178           5          3             1.0        0.647472\n",
      "179           5          2             1.0        0.650792\n",
      "180           5          5             1.0        0.651691\n",
      "181           5          9             1.0        0.660120\n",
      "182           4          2             1.0        0.664222\n",
      "183           5          5             1.0        0.666522\n",
      "184           5          3             1.0        0.670468\n",
      "185           5          3             1.0        0.680167\n",
      "186           5          6             1.0        0.681880\n",
      "187           5          3             0.0        0.681937\n",
      "188           5         10             0.0        0.693039\n",
      "189           5          4             1.0        0.694613\n",
      "190           5          3             1.0        0.703685\n",
      "191           5          1             0.5        0.727406\n",
      "192           5          2             1.0        0.729001\n",
      "193           5          2             1.0        0.736485\n",
      "194           5          6             1.0        0.777594\n",
      "195           5          2             1.0        0.789284\n",
      "196           5          2             1.0        0.789284\n",
      "197           5          9             1.0        0.794211\n",
      "198           5          9             1.0        0.796459\n",
      "199           5          6             1.0        0.825006\n",
      "200           5          5             1.0        0.848620\n",
      "201           4          8             0.5        0.876946\n",
      "202           5          3             1.0        0.950245\n",
      "203           5          3             0.5        0.950245\n",
      "204           4         11             1.0        0.970350\n",
      "205           5          6             0.5        0.983344\n",
      "206           5          3             0.5        0.984504\n",
      "207           4         10             1.0        0.987533\n",
      "208           5          4             1.0        1.015807\n",
      "209           5          8             1.0        1.034916\n",
      "210           5          6             1.0        1.080510\n",
      "211           5          5             1.0        1.112476\n",
      "212           4         11             1.0        1.117629\n",
      "213           5          4             1.0        1.132486\n",
      "214           5          1             1.0        1.165488\n",
      "215           5          1             1.0        1.165488\n",
      "216           5          7             1.0        1.173869\n",
      "217           5          3             1.0        1.175564\n",
      "218           5          2             1.0        1.204502\n",
      "219           5          5             1.0        1.213158\n",
      "220           5          1             1.0        1.213942\n",
      "221           4         14             1.0        1.252075\n",
      "222           5          2             1.0        1.271260\n",
      "223           5          3             1.0        1.294011\n",
      "224           5          4             1.0        1.362262\n",
      "225           5          2             1.0        1.379530\n",
      "226           5          2             1.0        1.379530\n",
      "227           5          6             0.5        1.395646\n",
      "228           5          3             0.0        1.405697\n",
      "229           5          9             1.0        1.413723\n",
      "230           5          2             1.0        1.422714\n",
      "231           5          7             1.0        1.425077\n",
      "232           5          5             1.0        1.496927\n",
      "233           5          9             0.5        1.527797\n",
      "234           5          2             1.0        1.593301\n",
      "235           5          2             0.5        1.593301\n",
      "236           5          3             1.0        1.630900\n",
      "237           4          5             1.0        1.769818\n",
      "238           5          1             1.0        1.837082\n",
      "239           5          5             1.0        1.881806\n",
      "240           5          3             1.0        1.943862\n",
      "241           5          3             1.0        1.943862\n",
      "242           5          2             1.0        2.052932\n",
      "243           5          7             1.0        2.055556\n",
      "244           5          2             1.0        2.066294\n",
      "245           5          2             1.0        2.066294\n",
      "246           5          2             0.5        2.066294\n",
      "247           5          5             1.0        2.098547\n",
      "248           5          1             1.0        2.220387\n",
      "249           5          2             1.0        2.295086\n",
      "250           5          2             0.5        2.333013\n",
      "251           5          1             1.0        2.494902\n",
      "252           5          3             1.0        2.814818\n",
      "253           5          1             1.0        2.924393\n",
      "254           5          1             1.0        2.924393\n",
      "255           5          1             1.0        2.924393\n",
      "256           5          3             0.0        3.264579\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.knn import KNN, WeightedKNN\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.options.display.max_rows = 999\n",
    "\n",
    "\n",
    "reviews_sentiment = pd.read_csv('datasets/reviews_sentiment.csv', delimiter=\";\")\n",
    "\n",
    "for i, row in reviews_sentiment.iterrows():\n",
    "    wc = len(row['reviewText'].strip().split(' '))\n",
    "    if wc != row['wordcount']:\n",
    "#         print(f\"El registro {i} tiene el campo wordcount incorrecto, corrigiendo...\")\n",
    "        reviews_sentiment.at[i,'wordcount'] = wc\n",
    "\n",
    "# print(sentiments)\n",
    "\n",
    "# cambiar positivo a 1, negativo a 0.\n",
    "reviews_sentiment['titleSentiment'].replace('positive', 1, inplace=True)\n",
    "reviews_sentiment['titleSentiment'].replace('negative', 0, inplace=True)\n",
    "\n",
    "ones = reviews_sentiment[reviews_sentiment['titleSentiment'] == 1]['titleSentiment'].aggregate('sum')\n",
    "ceroes = reviews_sentiment[reviews_sentiment['titleSentiment'] == 0]['titleSentiment'].aggregate('sum')\n",
    "\n",
    "prop = ones/len(reviews_sentiment)\n",
    "print(prop) \n",
    "\n",
    "reviews_sentiment['titleSentiment'].fillna(0.5, inplace=True)\n",
    "\n",
    "attributes = ['wordcount', 'titleSentiment', 'sentimentValue']\n",
    "target = ['starRating']\n",
    "\n",
    "# quitar los NaNs y verificar que no sea una porcion importante del conjunto de datos\n",
    "nan_title_sentiment = reviews_sentiment[reviews_sentiment['titleSentiment'].isna()]\n",
    "nan_text_sentiment = reviews_sentiment[reviews_sentiment['textSentiment'].isna()]\n",
    "print(f\"Cantidad de NaNs en titleSentiment {len(nan_title_sentiment)}\")\n",
    "print(f\"Cantidad de registros {len(reviews_sentiment)}\")\n",
    "reviews_sentiment = reviews_sentiment.dropna()\n",
    "print(f\"Cantidad de registros despues de sacar NaNs: {len(reviews_sentiment)}.\")\n",
    "\n",
    "reviews_sentiment = reviews_sentiment[target + attributes]\n",
    "\n",
    "print(reviews_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8697cbcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los comentarios valorados con 1 estrella, ¿que cantidad promedio de palabras tienen?\n",
      "El promedio es 8.16\n"
     ]
    }
   ],
   "source": [
    "# Pregunta a)\n",
    "print('Los comentarios valorados con 1 estrella, ¿que cantidad promedio de palabras tienen?')\n",
    "\n",
    "mean = reviews_sentiment[reviews_sentiment['starRating'] == 1]['wordcount'].aggregate('mean')\n",
    "print(f\"El promedio es {round(mean, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f16adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aux_functions import normalize_df\n",
    "\n",
    "\n",
    "register_class = np.array(reviews_sentiment.starRating)\n",
    "classes = np.array(reviews_sentiment.starRating.unique())\n",
    "\n",
    "# print(sentiments.star_rating.unique())\n",
    "\n",
    "# Se normalizan los datos por cada columna\n",
    "norm_data = np.array(normalize_df(reviews_sentiment[attributes])) # me queda un arreglo de registros con 3 atributos\n",
    "\n",
    "crossed_validation = 10\n",
    "batch_size = math.floor(len(norm_data)/crossed_validation)\n",
    "\n",
    "# valores de precision para cada corrido de validacion cruzada\n",
    "knn_precisions = np.zeros(crossed_validation)\n",
    "weight_knn_precisions = np.zeros(crossed_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17fcbc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.aux_functions import confusion_matrix\n",
    "from src.aux_functions import plot_matrix\n",
    "from src.aux_functions import plot_precision\n",
    "\n",
    "\n",
    "for i in range(crossed_validation):\n",
    "\n",
    "    # separo por lotes al conjunto de entrenamiento/testeo para la validacion cruzada\n",
    "    test_batch = np.array(range(batch_size * i, batch_size * (i + 1), 1))\n",
    "\n",
    "    # b) Dividir el conjunto de datos en un conjunto de entrenamiento y otro de prueba.\n",
    "    X = np.delete(norm_data, test_batch, axis = 0)\n",
    "    f_X = np.delete(register_class, test_batch, axis = 0)\n",
    "    Y = norm_data[test_batch[0]:(test_batch[-1] + 1)]\n",
    "    f_Y = register_class[test_batch[0]:(test_batch[-1] + 1)]\n",
    "\n",
    "    knn = KNN(X, f_X, classes)\n",
    "    weight_knn = WeightedKNN(X, f_X, classes)\n",
    "\n",
    "    predictions = knn.batch_classify(Y)\n",
    "    weight_predictions = weight_knn.batch_classify(Y)\n",
    "    \n",
    "    knn_confusion = confusion_matrix(predictions, f_Y, classes)\n",
    "    weight_knn_confusion = confusion_matrix(weight_predictions, f_Y, classes)\n",
    "\n",
    "    knn_precisions[i] = knn_confusion.trace()/knn_confusion.sum()\n",
    "    weight_knn_precisions[i] = weight_knn_confusion.trace()/weight_knn_confusion.sum()\n",
    "    plot_matrix(weight_knn_confusion, f'crossed_validation_{crossed_validation}_{i}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf241b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "plot_precision(knn_precisions, weight_knn_precisions, crossed_validation, f'crossed_validation_{crossed_validation}.png')\n",
    "\n",
    "print()\n",
    "# print(f'Para el KNN pesado, la precision promedio resulto: {w_knn_precisions.mean()} con un valor maximo de: {w_knn_precisions.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f8f9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn[6]: 0.6939153439153439\n",
      "knn_w[6]: 0.6687830687830686\n"
     ]
    }
   ],
   "source": [
    "from src.aux_functions import plot_precision_k\n",
    "\n",
    "knn_means = []\n",
    "weight_knn_means = []\n",
    "\n",
    "# neighbours = np.arange(start=3, stop=20, step=2)\n",
    "# for k in neighbours:\n",
    "\n",
    "# iterations = np.arange(start=1, stop=15, step=1)\n",
    "# for index in iterations:\n",
    "#     validation_range = neighbours = np.arange(start=3, stop=20, step=1)\n",
    "#     knn_precisions_avg = np.zeros(len(validation_range))\n",
    "#     weight_knn_precisions_avg = np.zeros(len(validation_range))\n",
    "\n",
    "validation_range = neighbours = np.arange(start=3, stop=25, step=1)\n",
    "for crossed_validation in validation_range:        \n",
    "\n",
    "    iterations = np.arange(start=0, stop=15, step=1)\n",
    "    knn_precisions_avg = np.zeros(len(iterations))\n",
    "    weight_knn_precisions_avg = np.zeros(len(iterations))\n",
    "    for index in iterations:\n",
    "\n",
    "        reviews_sentiment = reviews_sentiment.sample(frac=1).reset_index(drop=True)\n",
    "        register_class = np.array(reviews_sentiment.starRating)\n",
    "        classes = np.array(reviews_sentiment.starRating.unique())\n",
    "        # Se normalizan los datos por cada columna\n",
    "        norm_data = np.array(normalize_df(reviews_sentiment[attributes])) # me queda un arreglo de registros con 3 atributos\n",
    "        batch_size = math.floor(len(norm_data)/crossed_validation)\n",
    "\n",
    "        # valores de precision para cada corrido de validacion cruzada\n",
    "        knn_precisions = np.zeros(crossed_validation)\n",
    "        weight_knn_precisions = np.zeros(crossed_validation)\n",
    "\n",
    "        for i in range(crossed_validation):\n",
    "            # separo por lotes al conjunto de entrenamiento/testeo para la validacion cruzada\n",
    "            test_batch = np.array(range(batch_size * i, batch_size * (i + 1), 1))\n",
    "\n",
    "            # b) Dividir el conjunto de datos en un conjunto de entrenamiento y otro de prueba.\n",
    "            X = np.delete(norm_data, test_batch, axis = 0)\n",
    "            f_X = np.delete(register_class, test_batch, axis = 0)\n",
    "            Y = norm_data[test_batch[0]:(test_batch[-1] + 1)]\n",
    "            f_Y = register_class[test_batch[0]:(test_batch[-1] + 1)]\n",
    "\n",
    "            knn = KNN(X, f_X, classes)\n",
    "            weight_knn = WeightedKNN(X, f_X, classes)\n",
    "\n",
    "            predictions = knn.batch_classify(Y)\n",
    "            weight_predictions = weight_knn.batch_classify(Y)\n",
    "\n",
    "            knn_confusion = confusion_matrix(predictions, f_Y, classes)\n",
    "            weight_knn_confusion = confusion_matrix(weight_predictions, f_Y, classes)\n",
    "\n",
    "            knn_precisions[i] = knn_confusion.trace()/knn_confusion.sum()\n",
    "            weight_knn_precisions[i] = weight_knn_confusion.trace()/weight_knn_confusion.sum()\n",
    "        #     plot_matrix(weight_knn_confusion, f'crossed_validation_{crossed_validation}_{i}.png')\n",
    "\n",
    "\n",
    "        knn_precisions_avg[index] = knn_precisions.mean()\n",
    "        weight_knn_precisions_avg[index] = weight_knn_precisions.mean()\n",
    "\n",
    "#     print(knn_precisions_avg.mean())\n",
    "    knn_means.append(knn_precisions_avg.mean())\n",
    "    #knn_stds.append(knn_precisions_avg.std())\n",
    "    weight_knn_means.append(weight_knn_precisions_avg.mean())\n",
    "    #w_knn_stds.append(weight_knn_precisions_avg.std())\n",
    "print('knn[6]: ' + str(knn_means[6]))\n",
    "print('knn_w[6]: ' + str(weight_knn_means[6]))\n",
    "plot_precision_k(knn_means, weight_knn_means, validation_range, f'crossed_validation_knn_means.png')\n",
    "#     print(f'Finished neighbour {neigh_k}.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c331e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn[6]: 0.6901960784313724\n",
      "knn_w[6]: 0.652901484480432\n"
     ]
    }
   ],
   "source": [
    "print('knn[6]: ' + str(knn_means[12]))\n",
    "print('knn_w[6]: ' + str(weight_knn_means[10]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
